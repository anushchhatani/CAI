{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOofZGWM75EIcm6aoI3F9Sk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushchhatani/CAI/blob/main/transformer_pretraining_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "IYWZNjduyrID"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 50\n",
        "batch_size = 32\n",
        "data_path = \"user_behavior_dataset.csv\"  # Path to your dataset\n",
        "text_file_path = \"constructed_text_data.txt\"  # Temporary text file for vocab generation"
      ],
      "metadata": {
        "id": "8YUCNIXzywyW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "data = pd.read_csv(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Goo78pk5y_Ba",
        "outputId": "cbe282c9-0093-47fb-f6d6-f2dc2bed0bfc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct text sequences from the dataset\n",
        "print(\"Constructing text data...\")\n",
        "text_data = data.apply(\n",
        "    lambda row: (\n",
        "        f\"User with device {row['Device Model']} using {row['Operating System']} \"\n",
        "        f\"spends {row['App Usage Time (min/day)']} minutes daily on apps, \"\n",
        "        f\"has {row['Number of Apps Installed']} apps installed, \"\n",
        "        f\"and their data usage is {row['Data Usage (MB/day)']} MB per day.\"\n",
        "    ),\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrrFu3cezMAO",
        "outputId": "fd8577d5-7b39-4ec7-c4ee-59a7a8b062cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing text data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the constructed text data to a file\n",
        "print(\"Saving text data to file...\")\n",
        "text_data.to_csv(text_file_path, index=False, header=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY3x2EDwzY9n",
        "outputId": "65d07313-990c-477a-9500-0d8f77e9e2f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving text data to file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Determine Vocabulary Size and Train SentencePiece\n",
        "print(\"Counting unique tokens...\")\n",
        "with open(text_file_path, 'r') as f:\n",
        "    unique_token_count = len(set(f.read().split()))\n",
        "vocab_size = min(unique_token_count, 10000)  # Limit vocab size to a maximum of 10,000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JK_sNT1zbFl",
        "outputId": "4d5ac725-4f06-4094-c3f5-3c473dfacf8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting unique tokens...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training SentencePiece model with vocab size: {vocab_size}...\")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=text_file_path,\n",
        "    model_prefix=\"user_behavior_vocab\",\n",
        "    vocab_size=vocab_size,\n",
        "    model_type=\"word\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eJX_Inb0foy",
        "outputId": "513e1784-5a60-409f-a4e3-1e62e0f561bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SentencePiece model with vocab size: 889...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained SentencePiece model\n",
        "print(\"Loading SentencePiece model...\")\n",
        "sp = spm.SentencePieceProcessor(model_file=\"user_behavior_vocab.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4WCR6QO0nLG",
        "outputId": "00e077ee-c83d-43fc-b6eb-bd2433ff9690"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SentencePiece model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize KerasNLP Tokenizer\n",
        "# Include special tokens in the vocabulary\n",
        "special_tokens = {\"[UNK]\": 0, \"[CLS]\": 1, \"[SEP]\": 2}\n",
        "vocabulary = {sp.id_to_piece(i): i + len(special_tokens) for i in range(sp.get_piece_size())}\n",
        "vocabulary.update(special_tokens)  # Add special tokens to the vocabulary\n",
        "\n",
        "# Initialize the tokenizer with the updated vocabulary\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocabulary,\n",
        "    sequence_length=max_seq_length,\n",
        "    lowercase=True,\n",
        "    oov_token=\"[UNK]\",\n",
        ")"
      ],
      "metadata": {
        "id": "kGQn1ibm0onk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize Dataset\n",
        "# Tokenize the dataset without .to_tensor()\n",
        "print(\"Tokenizing dataset...\")\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(text)\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices(text_data).map(\n",
        "    tokenize_function, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").batch(batch_size).shuffle(10000).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmwCUiZd1JwK",
        "outputId": "6a84b190-32aa-4b47-e59f-31b0558e48c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build a Simple Transformer Model\n",
        "print(\"Building Transformer model...\")\n",
        "from keras_nlp.layers import TransformerEncoder\n",
        "from tensorflow.keras.layers import Embedding, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "dff = 512\n",
        "dropout_rate = 0.1\n",
        "\n",
        "inputs = Input(shape=(None,), dtype=tf.int32)\n",
        "x = Embedding(input_dim=vocab_size, output_dim=d_model)(inputs)\n",
        "for _ in range(num_layers):\n",
        "    x = TransformerEncoder(\n",
        "        num_heads=num_heads,\n",
        "        intermediate_dim=dff,\n",
        "        dropout=dropout_rate\n",
        "    )(x)\n",
        "outputs = Dense(vocab_size)(x)\n",
        "\n",
        "model = Model(inputs, outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sv3v4tt1ee2",
        "outputId": "19ea381e-eeea-4119-b47e-5b9ac4bb25d9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Transformer model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Compile and Train the Model\n",
        "print(\"Compiling and training the model...\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_z1v6MR1xeo",
        "outputId": "21cafe62-5fc6-4d48-bb3f-9f444e25ddb9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling and training the model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Save the Pretrained Model\n",
        "print(\"Saving the pretrained model...\")\n",
        "model.save(\"pretrained_transformer_model.keras\")\n",
        "print(\"Pretraining complete. Model saved as 'pretrained_transformer_model'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y35cA2vM1zea",
        "outputId": "1f308803-8bfa-41d6-8caf-408bb3d58ac2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the pretrained model...\n",
            "Pretraining complete. Model saved as 'pretrained_transformer_model'.\n"
          ]
        }
      ]
    }
  ]
}